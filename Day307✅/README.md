# Day 307
# ğŸ§  PyTorch One-Page Cheat Sheet (Data Science)

## ğŸ“¦ Import Basics

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
```

---

## ğŸ”¢ Tensor (Core of PyTorch)

```python
x = torch.tensor([1,2,3], dtype=torch.float32)
x = torch.randn(10, 3)
x = torch.zeros(5)
```

âœ” Like NumPy array
âœ” Supports GPU
âœ” Supports gradients

```python
x.device          # cpu / cuda
x.shape
x.dtype
```

---

## ğŸ” NumPy â†” Tensor

```python
import numpy as np

t = torch.from_numpy(np_array)
n = t.numpy()
```

âš ï¸ Same memory share â€” be careful

---

## ğŸ§® Autograd (Gradient)

```python
x = torch.tensor(2.0, requires_grad=True)
y = x**2
y.backward()
print(x.grad)
```

ğŸ‘‰ No manual calculus needed
ğŸ‘‰ Used in backpropagation

---

## ğŸ—‚ï¸ Dataset & DataLoader

```python
class MyDataset(Dataset):
    def __len__(self): ...
    def __getitem__(self, idx): ...
```

```python
loader = DataLoader(dataset, batch_size=32, shuffle=True)
```

âœ” Batch training
âœ” Memory efficient

---

## ğŸ§  Model (Neural Network)

```python
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x):
        return self.fc(x)
```

ğŸ§© `__init__` â†’ layers
ğŸ§© `forward()` â†’ data flow

---

## ğŸ“‰ Loss Functions

```python
nn.MSELoss()              # Regression
nn.CrossEntropyLoss()     # Classification
nn.BCELoss()              # Binary
```

Loss = model kitna ghalat hai

---

## âš™ï¸ Optimizers

```python
optim.SGD(model.parameters(), lr=0.01)
optim.Adam(model.parameters(), lr=0.001)
```

Adam = default best choice ğŸ‘

---

## ğŸ” Training Loop (ğŸ”¥ MOST IMPORTANT)

```python
for x, y in loader:
    pred = model(x)
    loss = loss_fn(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### Yaad Rakho Order:

1. Forward
2. Loss
3. Zero grad
4. Backward
5. Step

---

## ğŸ§ª Train vs Eval Mode

```python
model.train()
model.eval()

with torch.no_grad():
    output = model(x)
```

âš ï¸ Testing mein gradients off karo

---

## ğŸš€ GPU Usage

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
x = x.to(device)
```

---

## ğŸ“Š Common Data Science Tasks

| Task           | PyTorch                |
| -------------- | ---------------------- |
| Regression     | Linear / NN            |
| Classification | Softmax + CE           |
| NLP            | Embedding, Transformer |
| CV             | CNN                    |
| Time Series    | LSTM / GRU             |

---

## âŒ Common Mistakes

âŒ Forget `zero_grad()`
âŒ Training full data at once
âŒ Mixing NumPy & Tensor
âŒ No `model.eval()` in testing

---
