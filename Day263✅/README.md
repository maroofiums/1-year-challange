

# ğŸ§  Day 263 â€” Model Optimization & Hyperparameter Tuning

---

## ğŸ¯ Goal

Aaj ka aim hai:

* Multiple ML models ko compare karna
* GridSearchCV se best hyperparameters nikalna
* Final tuned model evaluate karna

---

## âš™ï¸ Step 1 â€” Imports

```python
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
```

---

## ğŸ§© Step 2 â€” Preprocessing + Split

```python
le = LabelEncoder()
y = le.fit_transform(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
```

ğŸ“˜ *Scaling important hai for SVM, KNN, Logistic Regression.*

---

## ğŸ¤– Step 3 â€” Models Dictionary

```python
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier()
}
```

---

## ğŸ§® Step 4 â€” Parameter Grids

```python
param_grids = {
    "Logistic Regression": {"C": [0.1, 1, 10], "solver": ["liblinear", "lbfgs"]},
    "SVM": {"C": [0.1, 1, 10], "kernel": ["linear", "rbf"], "gamma": ["scale", "auto"]},
    "Decision Tree": {"criterion": ["gini", "entropy"], "max_depth": [3, 5, 7, None]},
    "Naive Bayes": {"var_smoothing": [1e-9, 1e-8, 1e-7]},
    "KNN": {"n_neighbors": [3, 5, 7, 9], "weights": ["uniform", "distance"]}
}
```

---

## ğŸ” Step 5 â€” GridSearchCV Loop

```python
best_models = {}

for name, model in models.items():
    print(f"\nğŸ”§ Tuning {name}...")
    grid = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)
    grid.fit(X_train, y_train)

    print(f"âœ… Best Params: {grid.best_params_}")
    print(f"â­ CV Score: {grid.best_score_:.4f}")

    best_models[name] = grid.best_estimator_
```

ğŸ§  *GridSearchCV sab combinations try karta hai aur best parameters return karta hai.*

---

## ğŸ§ª Step 6 â€” Evaluate on Test Data

```python
for name, model in best_models.items():
    print(f"\nğŸš€ Evaluating {name}...")
    y_pred = model.predict(X_test)
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
```

---

## ğŸ“Š Step 7 â€” Model Ranking

```python
results = [(name, accuracy_score(y_test, model.predict(X_test)))
           for name, model in best_models.items()]

results.sort(key=lambda x: x[1], reverse=True)

print("\nğŸ† Final Model Ranking:")
for name, acc in results:
    print(f"{name}: {acc:.4f}")
```

---

## ğŸ§  Learnings & Tips

| Concept          | Key Idea                          |
| ---------------- | --------------------------------- |
| GridSearchCV     | Tries all parameter combinations  |
| Cross-Validation | Ensures stable performance        |
| Scaling          | Crucial for distance-based models |
| Overfitting      | Prevent using CV & tuning         |
| Model Comparison | Always compare > 3 models         |
| Save Model       | Use `joblib.dump()` for reuse     |

---

## ğŸ’¡ Summary

> â€œModel optimization is like fine-tuning a guitar â€”
> same strings (algorithm), better harmony (performance).â€ ğŸ¸

---

