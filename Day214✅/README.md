# Day 214

---

## ðŸš¢ Titanic Survival Prediction API

FastAPI + Scikit-learn based API jo Titanic dataset ke upar trained model ka use karke **survival prediction** karta hai.

---

### ðŸ“‚ Project Structure

```
Day186/
â”‚â”€â”€ app/
â”‚   â”‚â”€â”€ main.py              # FastAPI app
â”‚   â”‚â”€â”€ best_model.pkl       # Saved pipeline (model + preprocessing)
â”‚   â”‚â”€â”€ __init__.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

### âš¡ Features

* REST API built with **FastAPI**
* Input features based on Titanic dataset:

  * `pclass` â†’ Ticket class (1, 2, 3)
  * `sex` â†’ "male" / "female"
  * `age` â†’ Passenger age
  * `sibsp` â†’ Number of siblings/spouses aboard
  * `parch` â†’ Number of parents/children aboard
  * `fare` â†’ Ticket fare
  * `embarked` â†’ Port of embarkation (`C`, `Q`, `S`)
  * `who` â†’ "man" / "woman" / "child"
* Returns:

  * `prediction` â†’ 0 = Did not survive, 1 = Survived
  * `probability` â†’ Survival probability (if available)

---

### ðŸ”§ Installation

1. Clone repo / copy project:

   ```bash
   git clone <repo-url>
   cd Day186/app
   ```

2. Create & activate virtual environment:

   ```bash
   python -m venv venv
   venv\Scripts\activate   # Windows
   source venv/bin/activate # Linux/Mac
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

---

### ðŸš€ Run API

Start server with Uvicorn:

```bash
uvicorn main:app --reload
```

API will run at:
ðŸ‘‰ [http://127.0.0.1:8000](http://127.0.0.1:8000)
ðŸ‘‰ Interactive Docs: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

### ðŸ“Œ Example Request (via Swagger UI or Postman)

#### Request (JSON)

```json
{
  "pclass": 3,
  "sex": "male",
  "age": 22,
  "sibsp": 1,
  "parch": 0,
  "fare": 7.25,
  "embarked": "S",
  "who": "man"
}
```

#### Response

```json
{
  "prediction": 0,
  "probability": 0.08
}
```

---

### ðŸ›  Training & Model Pipeline

* Model trained on Titanic dataset using **Scikit-learn**
* Preprocessing (`OneHotEncoder`, `StandardScaler`) + ML model combined in **Pipeline**
* Saved using:

  ```python
  import joblib
  joblib.dump(best_pipeline, "best_model.pkl")
  ```

---
