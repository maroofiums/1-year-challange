# Day 108

---

## ğŸ§  **1. Perceptron (Single-layer Neural Network)**

### ğŸ”¹ Definition:

A **Perceptron** is the **simplest form of a neural network**, used for **binary classification**.

### ğŸ”¹ Structure:

* **Inputs (xâ‚, xâ‚‚, ..., xâ‚™)**
* **Weights (wâ‚, wâ‚‚, ..., wâ‚™)**
* **Bias (b)**
* **Activation Function** (e.g., step function)

### ğŸ”¹ Formula:

$$
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
$$

Where:

* $f$ is usually a **step function**:

$$
f(z) =
\begin{cases}
1 & \text{if } z \geq 0 \\
0 & \text{otherwise}
\end{cases}
$$

### ğŸ”¹ Working:

1. Compute weighted sum $z = \sum w_i x_i + b$
2. Pass through activation function to get output (0 or 1)

### ğŸ”¹ Limitation:

Can only solve **linearly separable** problems (like AND, OR) â€” **NOT XOR**.

---

## ğŸ§  **2. Multilayer Perceptron (MLP)**

### ğŸ”¹ Definition:

A **Multilayer Perceptron** is a type of **Feedforward Neural Network** with one or more **hidden layers**.

### ğŸ”¹ Structure:

* **Input layer**
* **Hidden layer(s)** (one or more)
* **Output layer**

Each layer is made of **neurons (nodes)** connected with **weights**.

```
[Input Layer] â†’ [Hidden Layer(s)] â†’ [Output Layer]
```

### ğŸ”¹ Activation Functions:

Instead of a step function, we use non-linear activations:

* ReLU: $f(x) = \max(0, x)$
* Sigmoid: $f(x) = \frac{1}{1 + e^{-x}}$
* Tanh: $f(x) = \tanh(x)$

### ğŸ”¹ Forward Pass:

Each neuron computes:

$$
z = \sum w_i x_i + b,\quad a = \text{activation}(z)
$$

### ğŸ”¹ Backpropagation:

Used for **training**:

* Computes **loss** (e.g., MSE, Cross-Entropy)
* Uses **gradient descent** to adjust weights via **chain rule** (partial derivatives)

---

## ğŸ” Summary Table:

| Feature               | Perceptron      | MLP (Multilayer Perceptron) |
| --------------------- | --------------- | --------------------------- |
| Layers                | 1               | 2 or more                   |
| Can solve XOR?        | âŒ No            | âœ… Yes                       |
| Activation Function   | Step            | ReLU, Sigmoid, etc.         |
| Learning Algorithm    | Perceptron Rule | Backpropagation             |
| Non-linearity support | âŒ               | âœ…                           |

